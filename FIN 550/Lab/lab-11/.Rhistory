backward_step_model <- step(full_model,
direction = "backward")
# Show summary of the final backward stepwise selection model
summary(backward_step_model)
# Redo problem 2(a), using algorithm 6.1 from ISLAR
# Load required libraries
library(leaps)
library(caret)
library(dplyr)
# Load the Seatbelts dataset and create the kms2 variable
data("Seatbelts")
seatbelts <- as_tibble(Seatbelts) %>%
select(DriversKilled, law, PetrolPrice, kms) %>%
mutate(kms2 = kms^2)
# Redo problem 2(a), using algorithm 6.1 from ISLAR
# Load required libraries
library(leaps)
library(caret)
library(dplyr)
# Check the column names of the Seatbelts dataset
colnames(Seatbelts)
# Convert Seatbelts to a tibble and inspect the first few rows to ensure correct column names
seatbelts <- as_tibble(Seatbelts)
head(seatbelts)
# The correct column names for the predictors
# "DriversKilled", "law", "PetrolPrice", and "kms" should match exactly
# Step 1: Convert dataset into tibble and create kms2
seatbelts <- seatbelts %>%
select(DriversKilled, law, PetrolPrice, kms) %>%  # Select relevant columns
mutate(kms2 = kms^2)  # Create kms2 as the square of kms
# Redo problem 2(a), using algorithm 6.1 from ISLAR
# Load required libraries
library(leaps)
library(caret)
library(dplyr)
# Check the column names of the Seatbelts dataset to confirm correct names
colnames(Seatbelts)
# Convert Seatbelts to a tibble and inspect the first few rows
seatbelts <- as_tibble(Seatbelts)
head(seatbelts)
# Step 1: Select relevant columns and create kms2 (square of kms)
# Ensure the column names match exactly
seatbelts <- seatbelts %>%
select(DriversKilled, law, PetrolPrice, kms) %>%
mutate(kms2 = kms^2)
# Redo problem 2(a), using algorithm 6.1 from ISLAR
# Check the column names of the Seatbelts dataset to confirm correct names
colnames(Seatbelts)
# Convert Seatbelts to a tibble and inspect the first few rows
seatbelts <- as_tibble(Seatbelts)
head(seatbelts)
# Step 1: Select relevant columns and create kms2 (square of kms)
# Ensure the column names match exactly
seatbelts <- seatbelts %>%
select(DriversKilled, law, PetrolPrice, kms) %>%
mutate(kms2 = kms^2)
# Redo problem 2(a), using algorithm 6.1 from ISLAR
# Load required libraries
library(leaps)
library(caret)
library(dplyr)
# Check the column names of the Seatbelts dataset to confirm correct names
colnames(Seatbelts)
# Convert Seatbelts to a tibble and inspect the first few rows
seatbelts <- as_tibble(Seatbelts)
head(seatbelts)
# Step 1: Select relevant columns and create kms2 (square of kms)
# Explicitly using dplyr::select to avoid function masking issues
seatbelts <- seatbelts %>%
dplyr::select(DriversKilled, law, PetrolPrice, kms) %>%  # Ensure correct columns
mutate(kms2 = kms^2)  # Create kms2 as the square of kms
# Confirm the dataset structure
head(seatbelts)
# Step 2: Perform Best Subset Selection using regsubsets
x <- seatbelts %>% dplyr::select(law, PetrolPrice, kms, kms2)  # Define predictors
y <- seatbelts$DriversKilled  # Define outcome variable
# Perform Best Subset Selection using the regsubsets function
best_subset <- regsubsets(x = as.matrix(x), y = y, nbest = 1, nvmax = 4, method = "exhaustive")
best_subset_summary <- summary(best_subset)
# Step 3: Use R-squared to select the best model among models with the same number of predictors
rsq_values <- best_subset_summary$rsq
best_rsq_model_size <- which.max(rsq_values)  # Find the best model based on R-squared
best_rsq_model_size  # Print the best model based on R-squared
# Step 4: Use cross-validation to compare models with different numbers of predictors
set.seed(3)
cv_ctrl <- trainControl(method = "cv", number = 5)
# Fit the model using cross-validation
lm_cv_model <- train(DriversKilled ~ law + PetrolPrice + kms + kms2,
data = seatbelts, method = "lm", trControl = cv_ctrl)
# Show summary of the cross-validated model
summary(lm_cv_model)
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
# (a) How many possible models are there?
total_models <- 2^22
total_models
# (b)	You consider using Best Subset Selection to choose a model. How many models will this procedure consider?
best_subset_models <- 2^22
best_subset_models
# (c) You consider using Forward Stepwise Selection.  How many models will this procedure consider?
forward_stepwise_models <- (22 * (22 + 1)) / 2
forward_stepwise_models
# Load the dataset into a tibble. Keep the outcome variable and the predictors law, PetrolPrice, and kms. Create the variable kms2 = kms^2
library(leaps)
library(caret)
library(dplyr)  # For data manipulation
# Load the built-in Seatbelts dataset
data("Seatbelts")
# Convert it to a tibble and create kms2
seatbelts <- as_tibble(Seatbelts)
# Select the variables: law, PetrolPrice, kms, and create kms2
seatbelts <- seatbelts %>%
select(DriversKilled, law, PetrolPrice, kms) %>%
mutate(kms2 = kms^2)
# Check the first few rows to confirm the data
head(seatbelts)
# (a) Best subset selection with 5-fold cross-validation
set.seed(3)
# Define predictors (X) and outcome (Y)
x <- seatbelts %>% select(law, PetrolPrice, kms, kms2)
y <- seatbelts$DriversKilled
# Perform Best Subset Selection
best_subset <- regsubsets(x = as.matrix(x), y = y, nbest = 1, nvmax = 4, method = "exhaustive")
# 5-fold cross-validation using caret
ctrl <- trainControl(method = "cv", number = 5)
lm_model <- train(DriversKilled ~ law + PetrolPrice + kms + kms2, data = seatbelts, method = "lm", trControl = ctrl)
# Show summary of the model
summary(lm_model)
# (b) Forward Stepwise Selection with 5-fold cross-validation
# Load required libraries
library(MASS)
library(caret)
# Set seed for reproducibility
set.seed(3)
# Define the full and null models for forward stepwise selection
full_model <- lm(DriversKilled ~ law + PetrolPrice + kms + kms2, data = seatbelts)
null_model <- lm(DriversKilled ~ 1, data = seatbelts)  # Null model (no predictors)
# Perform forward stepwise selection
forward_step_model <- step(null_model,
scope = list(lower = null_model, upper = full_model),
direction = "forward")
# Show summary of the final forward stepwise selection model
summary(forward_step_model)
# (c) Backward Stepwise Selection with LOOCV and 5-fold cross-validation
set.seed(3)
# Perform backward stepwise selection
backward_step_model <- step(full_model,
direction = "backward")
# Show summary of the final backward stepwise selection model
summary(backward_step_model)
# Redo problem 2(a), using algorithm 6.1 from ISLAR
# Load required libraries
library(leaps)
library(caret)
library(dplyr)
# Check the column names of the Seatbelts dataset to confirm correct names
colnames(Seatbelts)
# Convert Seatbelts to a tibble and inspect the first few rows
seatbelts <- as_tibble(Seatbelts)
head(seatbelts)
# Step 1: Select relevant columns and create kms2 (square of kms)
# Explicitly using dplyr::select to avoid function masking issues
seatbelts <- seatbelts %>%
dplyr::select(DriversKilled, law, PetrolPrice, kms) %>%  # Ensure correct columns
mutate(kms2 = kms^2)  # Create kms2 as the square of kms
# Confirm the dataset structure
head(seatbelts)
# Step 2: Perform Best Subset Selection using regsubsets
x <- seatbelts %>% dplyr::select(law, PetrolPrice, kms, kms2)  # Define predictors
y <- seatbelts$DriversKilled  # Define outcome variable
# Perform Best Subset Selection using the regsubsets function
best_subset <- regsubsets(x = as.matrix(x), y = y, nbest = 1, nvmax = 4, method = "exhaustive")
best_subset_summary <- summary(best_subset)
# Step 3: Use R-squared to select the best model among models with the same number of predictors
rsq_values <- best_subset_summary$rsq
best_rsq_model_size <- which.max(rsq_values)  # Find the best model based on R-squared
best_rsq_model_size  # Print the best model based on R-squared
# Step 4: Use cross-validation to compare models with different numbers of predictors
set.seed(3)
cv_ctrl <- trainControl(method = "cv", number = 5)
# Fit the model using cross-validation
lm_cv_model <- train(DriversKilled ~ law + PetrolPrice + kms + kms2,
data = seatbelts, method = "lm", trControl = cv_ctrl)
# Show summary of the cross-validated model
summary(lm_cv_model)
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
# (a) How many possible models are there?
total_models <- 2^22
total_models
# (b)	You consider using Best Subset Selection to choose a model. How many models will this procedure consider?
best_subset_models <- 2^22
best_subset_models
# (c) You consider using Forward Stepwise Selection.  How many models will this procedure consider?
forward_stepwise_models <- (22 * (22 + 1)) / 2
forward_stepwise_models
# Load the dataset into a tibble. Keep the outcome variable and the predictors law, PetrolPrice, and kms. Create the variable kms2 = kms^2
library(leaps)
library(caret)
library(dplyr)  # For data manipulation
# Load the built-in Seatbelts dataset
data("Seatbelts")
# Convert it to a tibble and create kms2
seatbelts <- as_tibble(Seatbelts)
# Select the variables: law, PetrolPrice, kms, and create kms2
seatbelts <- seatbelts %>%
select(DriversKilled, law, PetrolPrice, kms) %>%
mutate(kms2 = kms^2)
# Check the first few rows to confirm the data
head(seatbelts)
# (a) Best subset selection with 5-fold cross-validation
set.seed(3)
# Define predictors (X) and outcome (Y)
x <- seatbelts %>% select(law, PetrolPrice, kms, kms2)
y <- seatbelts$DriversKilled
# Perform Best Subset Selection
best_subset <- regsubsets(x = as.matrix(x), y = y, nbest = 1, nvmax = 4, method = "exhaustive")
# 5-fold cross-validation using caret
ctrl <- trainControl(method = "cv", number = 5)
lm_model <- train(DriversKilled ~ law + PetrolPrice + kms + kms2, data = seatbelts, method = "lm", trControl = ctrl)
# Show summary of the model
summary(lm_model)
# (b) Forward Stepwise Selection with 5-fold cross-validation
# Load required libraries
library(MASS)
library(caret)
# Set seed for reproducibility
set.seed(3)
# Define the full and null models for forward stepwise selection
full_model <- lm(DriversKilled ~ law + PetrolPrice + kms + kms2, data = seatbelts)
null_model <- lm(DriversKilled ~ 1, data = seatbelts)  # Null model (no predictors)
# Perform forward stepwise selection
forward_step_model <- step(null_model,
scope = list(lower = null_model, upper = full_model),
direction = "forward")
# Show summary of the final forward stepwise selection model
summary(forward_step_model)
# (c) Backward Stepwise Selection with LOOCV and 5-fold cross-validation
set.seed(3)
# Perform backward stepwise selection
backward_step_model <- step(full_model,
direction = "backward")
# Show summary of the final backward stepwise selection model
summary(backward_step_model)
# Redo problem 2(a), using algorithm 6.1 from ISLAR
# Load required libraries
library(leaps)
library(caret)
library(dplyr)
# Check the column names of the Seatbelts dataset to confirm correct names
colnames(Seatbelts)
# Convert Seatbelts to a tibble and inspect the first few rows
seatbelts <- as_tibble(Seatbelts)
head(seatbelts)
# Step 1: Select relevant columns and create kms2 (square of kms)
# Explicitly using dplyr::select to avoid function masking issues
seatbelts <- seatbelts %>%
dplyr::select(DriversKilled, law, PetrolPrice, kms) %>%  # Ensure correct columns
mutate(kms2 = kms^2)  # Create kms2 as the square of kms
# Confirm the dataset structure
head(seatbelts)
# Step 2: Perform Best Subset Selection using regsubsets
x <- seatbelts %>% dplyr::select(law, PetrolPrice, kms, kms2)  # Define predictors
y <- seatbelts$DriversKilled  # Define outcome variable
# Perform Best Subset Selection using the regsubsets function
best_subset <- regsubsets(x = as.matrix(x), y = y, nbest = 1, nvmax = 4, method = "exhaustive")
best_subset_summary <- summary(best_subset)
# Step 3: Use R-squared to select the best model among models with the same number of predictors
rsq_values <- best_subset_summary$rsq
best_rsq_model_size <- which.max(rsq_values)  # Find the best model based on R-squared
best_rsq_model_size  # Print the best model based on R-squared
# Step 4: Use cross-validation to compare models with different numbers of predictors
set.seed(3)
cv_ctrl <- trainControl(method = "cv", number = 5)
# Fit the model using cross-validation
lm_cv_model <- train(DriversKilled ~ law + PetrolPrice + kms + kms2,
data = seatbelts, method = "lm", trControl = cv_ctrl)
# Show summary of the cross-validated model
summary(lm_cv_model)
library(tidyverse)
library(boot)
# Load the data into a data frame
seatbelts <- as_tibble(datasets::Seatbelts)
# We will consider 4 possible predictors of DriversKilled:
# X1 = law
# X2 = PetrolPrice
# X3 = kms
# X4 = kms^2
seatbelts <- seatbelts %>%
select(DriversKilled, law, PetrolPrice, kms) %>%
mutate(kms2 = kms^2)
# How many observations are there? Does that make sense?
nrow(seatbelts)
# What is the range of the outcome variable, `DriversKilled`?
summary(seatbelts$DriversKilled)
# M0: constant only model (i.e. no predictors)
f0_1 <- formula(DriversKilled ~ 1)
f0_1
# Same as: `lm(DriversKilled ~ 1, data=seatbelts)`
lm(f0_1, data = seatbelts)
# M1: all models with 1 predictor
f1_1 <- formula(DriversKilled ~ law)
f1_2 <- formula(DriversKilled ~ PetrolPrice)
f1_3 <- formula(DriversKilled ~ kms)
f1_4 <- formula(DriversKilled ~ kms2)
# M2: all models with 2 predictors
f2_1 <- formula(DriversKilled ~ law + PetrolPrice)
f2_2 <- formula(DriversKilled ~ law + kms)
f2_3 <- formula(DriversKilled ~ law + kms2)
f2_4 <- formula(DriversKilled ~ PetrolPrice + kms)
f2_5 <- formula(DriversKilled ~ PetrolPrice + kms2)
f2_6 <- formula(DriversKilled ~ kms + kms2)
# M3: all models with 3 predictors
f3_1 <- formula(DriversKilled ~ PetrolPrice + kms + kms2)
f3_2 <- formula(DriversKilled ~ law + kms + kms2)
f3_3 <- formula(DriversKilled ~ law + PetrolPrice + kms2)
f3_4 <- formula(DriversKilled ~ law + PetrolPrice + kms)
# M4: all models with 4 predictors
f4_1 <- formula(DriversKilled ~ law + PetrolPrice + kms + kms2)
# Use glm() to estimate linear regression of formula f on the seatbelts dataset
cv_fun <- function(f) {
glmfit <- glm(f, data = seatbelts)
cv.glm(data = seatbelts, glmfit)$delta[1]
}
# Run the function
cv_fun(f0_1)
cv_fun(f3_4)
# Create a list of formulas
formulas <- list(f0_1,
f1_1, f1_2, f1_3, f1_4,
f2_1, f2_2, f2_3, f2_4, f2_5, f2_6,
f3_1, f3_2, f3_3, f3_4,
f4_1)
# Create a vector for storing the LOOCV MSE for formulas
formulas_cv <- vector("numeric", length(formulas))
# Use the function we created to calculate the LOOCV MSE for each formula
for (i in 1:length(formulas)) {
formulas_cv[[i]] <- cv_fun(formulas[[i]])
}
best_model <- which.min(formulas_cv)
# Formula
formulas[[best_model]]
# MSE
formulas_cv[[best_model]]
# Instead of writing this for loop:
#    for (i in 1:length(formulas)) {
#      formulas_cv[[i]] <- cv_fun(formulas[[i]])
#    }
# We can use lapply():
formulas_cv2 <- lapply(formulas, cv_fun)
best_model <- which.min(formulas_cv2)
formulas[[best_model]]
# lapply() returns a list. Recall: formulas_cv2 <- lapply(formulas, cv_fun)
typeof(formulas_cv2)
# sapply() works like lapply(), but it simplifies the output
formulas_cv3 <- sapply(formulas, cv_fun)
typeof(formulas_cv3)
all.equal(simplify(formulas_cv2), formulas_cv3)
# Our model had 4 possible predictors: 2^4 = 16 models
length(formulas)
# 40 predictors: 2^40 = over 1 trillion models
prettyNum(2^40, big.mark = ",")
# 300 predictors: 2.04 x 10^90 models!
format(2^300, scientific=TRUE)
# Initialize vector and list to store MSE and formula for each model M (M0-M4)
forward_cv <- vector("numeric", 5)
forward_formulas <- vector(mode = "list", 5)
# M0: no predictors
forward_cv[1] <- cv_fun(f0_1)
forward_formulas[[1]] <- f0_1
# M1: consider all 1-variable models
M_formulas <- list(f1_1, f1_2, f1_3, f1_4)
cv <- sapply(M_formulas, cv_fun)
forward_cv[2] <- min(cv)
# Store smallest MSE for M1
forward_formulas[[2]] <- M_formulas[[which.min(cv)]]
# Store best model for M1
forward_formulas[[2]]
# Display the 1-variable model with the lowest MSE
# M2: consider all 2-variable models that include PetrolPrice
M_formulas <- list(f2_1, f2_4, f2_5)
print(as.character(M_formulas))
cat("\n")
cv <- sapply(M_formulas, cv_fun)
forward_cv[3] <- min(cv)
forward_formulas[[3]] <- M_formulas[[which.min(cv)]]
forward_formulas[[3]]
# Display the 2-variable model with the lowest MSE
#M3: consider all 3-variable models that include PetrolPrice and kms2
M_formulas <- list(f3_1, f3_3)
print(as.character(M_formulas))
cat("\n")
cv <- sapply(M_formulas, cv_fun)
forward_cv[4] <- min(cv)
forward_formulas[[4]] <- M_formulas[[which.min(cv)]]
forward_formulas[[4]]
# Display the 3-variable model with the lowest MSE
# M4: all predictors (4-variable model)
forward_cv[5] <- cv_fun(f4_1)
forward_formulas[[5]] <- f4_1
# Display MSE and formulas for models M0-M4
forward_cv
cat("\n")
print(as.character(forward_formulas))
cat("\n")
# Display model with the lowest cross-validated mean-squared error
forward_formulas[[which.min(forward_cv)]]
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
# (a) How many possible models are there?
total_models <- 2^22
total_models
# (b)	You consider using Best Subset Selection to choose a model. How many models will this procedure consider?
best_subset_models <- 2^22
best_subset_models
# (c) You consider using Forward Stepwise Selection.  How many models will this procedure consider?
forward_stepwise_models <- (22 * (22 + 1)) / 2
forward_stepwise_models
# Load the dataset into a tibble. Keep the outcome variable and the predictors law, PetrolPrice, and kms. Create the variable kms2 = kms^2
library(leaps)
library(caret)
library(dplyr)
# Load the built-in Seatbelts dataset
data("Seatbelts")
# Convert it to a tibble and create kms2
seatbelts <- as_tibble(Seatbelts)
# Select the variables: law, PetrolPrice, kms, and create kms2
seatbelts <- seatbelts %>%
select(DriversKilled, law, PetrolPrice, kms) %>%
mutate(kms2 = kms^2)
# Check the first few rows to confirm the data
head(seatbelts)
# (a) Best subset selection with 5-fold cross-validation
set.seed(3)
# Define predictors (X) and outcome (Y)
x <- seatbelts %>% select(law, PetrolPrice, kms, kms2)
y <- seatbelts$DriversKilled
# Perform Best Subset Selection
best_subset <- regsubsets(x = as.matrix(x), y = y, nbest = 1, nvmax = 4, method = "exhaustive")
# 5-fold cross-validation using caret
ctrl <- trainControl(method = "cv", number = 5)
lm_model <- train(DriversKilled ~ law + PetrolPrice + kms + kms2, data = seatbelts, method = "lm", trControl = ctrl)
# Show summary of the model
summary(lm_model)
# (b) Forward Stepwise Selection with 5-fold cross-validation
# Load required libraries
library(MASS)
library(caret)
# Set seed for reproducibility
set.seed(3)
# Define the full and null models for forward stepwise selection
full_model <- lm(DriversKilled ~ law + PetrolPrice + kms + kms2, data = seatbelts)
null_model <- lm(DriversKilled ~ 1, data = seatbelts)  # Null model (no predictors)
# Perform forward stepwise selection
forward_step_model <- step(null_model,
scope = list(lower = null_model, upper = full_model),
direction = "forward")
# Show summary of the final forward stepwise selection model
summary(forward_step_model)
# (c) Backward Stepwise Selection with LOOCV and 5-fold cross-validation
set.seed(3)
# Perform backward stepwise selection
backward_step_model <- step(full_model,
direction = "backward")
# Show summary of the final backward stepwise selection model
summary(backward_step_model)
# Redo problem 2(a), using algorithm 6.1 from ISLAR
# Load required libraries
library(leaps)
library(caret)
library(dplyr)
# Check the column names of the Seatbelts dataset to confirm correct names
colnames(Seatbelts)
# Convert Seatbelts to a tibble and inspect the first few rows
seatbelts <- as_tibble(Seatbelts)
head(seatbelts)
# Step 1: Select relevant columns and create kms2 (square of kms)
# Explicitly using dplyr::select to avoid function masking issues
seatbelts <- seatbelts %>%
dplyr::select(DriversKilled, law, PetrolPrice, kms) %>%  # Ensure correct columns
mutate(kms2 = kms^2)  # Create kms2 as the square of kms
# Confirm the dataset structure
head(seatbelts)
# Step 2: Perform Best Subset Selection using regsubsets
x <- seatbelts %>% dplyr::select(law, PetrolPrice, kms, kms2)  # Define predictors
y <- seatbelts$DriversKilled  # Define outcome variable
# Perform Best Subset Selection using the regsubsets function
best_subset <- regsubsets(x = as.matrix(x), y = y, nbest = 1, nvmax = 4, method = "exhaustive")
best_subset_summary <- summary(best_subset)
# Step 3: Use R-squared to select the best model among models with the same number of predictors
rsq_values <- best_subset_summary$rsq
best_rsq_model_size <- which.max(rsq_values)  # Find the best model based on R-squared
best_rsq_model_size  # Print the best model based on R-squared
# Step 4: Use cross-validation to compare models with different numbers of predictors
set.seed(3)
cv_ctrl <- trainControl(method = "cv", number = 5)
# Fit the model using cross-validation
lm_cv_model <- train(DriversKilled ~ law + PetrolPrice + kms + kms2,
data = seatbelts, method = "lm", trControl = cv_ctrl)
# Show summary of the cross-validated model
summary(lm_cv_model)
# (b) Forward Stepwise Selection with 5-fold cross-validation
# Load required libraries
library(caret)
# Set seed for reproducibility
set.seed(3)
# Define the full and null models for forward stepwise selection
full_model <- lm(DriversKilled ~ law + PetrolPrice + kms + kms2, data = seatbelts)
null_model <- lm(DriversKilled ~ 1, data = seatbelts)  # Null model (no predictors)
# Perform forward stepwise selection
forward_step_model <- step(null_model,
scope = list(lower = null_model, upper = full_model),
direction = "forward")
# Show summary of the final forward stepwise selection model
summary(forward_step_model)
