y_hat_mean = mean(y_hat),
f_mean = mean(f),
mse = mean((y - y_hat)^2),
noise = mean(epsilon^2),
bias_squared = mean((f - y_hat_mean)^2),
variance = var(y_hat)
)
# Calculate the average MSE for the linear model (deg = 1)
mse_linear <- mean(f_hat_sum_linear$mse)
print(paste("Average MSE for linear function: ", round(mse_linear, 3)))
# Compare bias and variance for cubic and linear models
bias_squared_cubic <- mean(f_hat_sum$bias_squared)
variance_cubic <- mean(f_hat_sum$variance)
print(paste("Average bias^2 for cubic model: ", round(bias_squared_cubic, 3)))
print(paste("Average variance for cubic model: ", round(variance_cubic, 3)))
bias_squared_linear <- mean(f_hat_sum_linear$bias_squared)
variance_linear <- mean(f_hat_sum_linear$variance)
print(paste("Average bias^2 for linear model: ", round(bias_squared_linear, 3)))
print(paste("Average variance for linear model: ", round(variance_linear, 3)))
library(tidyverse)
# Define f, the conditional expectation function (ground truth)
f <- function(x) -16 + 24*x - 9*x^2 + x^3
ggplot() +
geom_function(fun = f, xlim = c(0, 6), color = "orange", linewidth = 1.5)
# Define draw, a function that draws a data sample
#   at    A vector of predictors x. If NULL (the default), predictors are drawn randomly between 0 and 6
#   sd    The standard deviation of epsilon
draw <- function(at = NULL, n, sd, fun = f) {
if (is.null(at)) at <- runif(n = n, min = 0, max = 6)
tibble(x = at,
f = f(x),
epsilon = rnorm(n = length(x), mean = 0, sd = sd),
y = f(x) + epsilon)
}
# `draw` mode 1: draw a sample of 5 observations, using random values of x
set.seed(123)  # Setting seed for reproducibility
draw1 <- draw(n = 5, sd = 0)
print(draw1)
# `draw` mode 2: draw a sample where the predictors are fixed at values from 0 to 6 in increments of 0.5
draw2 <- draw(at = seq(0, 6, by = 0.5), n = NULL, sd = 10)
print(draw2)
# Plot f and y with a data sample of size 13, no noise (sd = 0)
no_noise_data <- draw(at = seq(0, 6, by = 0.5), n = NULL, sd = 0)
ggplot(no_noise_data, aes(x = x)) +
geom_line(aes(y = f), color = "orange", size = 1.5) +
geom_point(aes(y = y), color = "black", size = 3) +
ggtitle("No noise: f(x) vs y")
# Plot f and y with a data sample of size 13, more noise (sd = 10)
noise_data <- draw(at = seq(0, 6, by = 0.5), n = NULL, sd = 10)
ggplot(noise_data, aes(x = x)) +
geom_line(aes(y = f), color = "orange", size = 1.5) +
geom_point(aes(y = y), color = "black", size = 3) +
ggtitle("With noise: f(x) vs y")
# Define f_hat, the machine learning algorithm.
# Output
#   A data frame consisting of x from test_data and the corresponding predictions y_hat = f_hat(x)
f_hat <- function(test_data, training_data, deg) {
y_hat <- lm(y ~ poly(x, degree = deg, raw = TRUE), data = training_data) %>%
predict.lm(newdata = test_data)
mutate(test_data, y_hat = y_hat)
}
# Plot f with a training data and test data predictions
# Training and test data for no noise case
test_data <- draw(at = seq(0, 6, by = 0.5), n = NULL, sd = 0)
training_data <- draw(n = 10, sd = 0)
# Estimating cubic model (degree = 3)
f_hat_no_noise <- f_hat(test_data = test_data, training_data = training_data, deg = 3)
# Plotting
ggplot(test_data, aes(x = x)) +
geom_line(aes(y = f), color = "orange", size = 1.5) +
geom_line(data = f_hat_no_noise, aes(y = y_hat), color = "blue", size = 1.5, alpha = 0.5) +
geom_point(aes(y = y), color = "black", size = 3) +
ggtitle("No Noise: f(x) vs ML predictions (degree 3)")
# Plot f with a training data and test data predictions
# Training and test data for noise case
test_data_noise <- draw(at = seq(0, 6, by = 0.5), n = NULL, sd = 10)
training_data_noise <- draw(n = 10, sd = 10)
# Estimating cubic model (degree = 3)
f_hat_noise <- f_hat(test_data = test_data_noise, training_data = training_data_noise, deg = 3)
# Plotting
ggplot(test_data_noise, aes(x = x)) +
geom_line(aes(y = f), color = "orange", size = 1.5) +
geom_line(data = f_hat_noise, aes(y = y_hat), color = "blue", size = 1.5, alpha = 0.5) +
geom_point(aes(y = y), color = "black", size = 3) +
ggtitle("Noise (sd = 10): f(x) vs ML predictions (degree 3)")
# How many iterations (start with 2 iterations, then increase to 1,000 once the code is working)
iters <- 1000  # Number of iterations
f_hat_list <- vector("list", iters)
for (i in 1:iters) {
training_data <- draw(n = 10, sd = 10)
f_hat_list[[i]] <- f_hat(test_data = test_data_noise, training_data = training_data, deg = 3)
}
# Combine the results into one long data frame
f_hat_combined <- bind_rows(f_hat_list)
f_hat_sum <- f_hat_combined %>%
group_by(x) %>%
summarise(
y_hat_mean = mean(y_hat),
f_mean = mean(f),
mse = mean((y - y_hat)^2),
noise = mean(epsilon^2),
bias_squared = mean((f - y_hat_mean)^2),
variance = var(y_hat)
)
print(f_hat_sum)
ggplot(f_hat_sum, aes(x = x)) +
geom_line(aes(y = f_mean), color = "orange", size = 1.5) +
geom_line(aes(y = y_hat_mean), color = "blue", size = 1.5, alpha = 0.5) +
ggtitle("Summary of Predictions: f(x) vs y_hat_mean")
# Average MSE when estimating f as a cubic function of x
mse_cubic <- mean(f_hat_sum$mse)
print(paste("Average MSE for cubic function: ", round(mse_cubic, 3)))
# Repeat the ML process for degree = 1
f_hat_list_linear <- vector("list", iters)
for (i in 1:iters) {
training_data <- draw(n = 10, sd = 10)
f_hat_list_linear[[i]] <- f_hat(test_data = test_data_noise, training_data = training_data, deg = 1)
}
# Combine the results into one long data frame
f_hat_combined_linear <- bind_rows(f_hat_list_linear)
# Summarise the results for the linear model (deg = 1)
f_hat_sum_linear <- f_hat_combined_linear %>%
group_by(x) %>%
summarise(
y_hat_mean = mean(y_hat),
f_mean = mean(f),
mse = mean((y - y_hat)^2),
noise = mean(epsilon^2),
bias_squared = mean((f - y_hat_mean)^2),
variance = var(y_hat)
)
# Calculate the average MSE for the linear model (deg = 1)
mse_linear <- mean(f_hat_sum_linear$mse)
print(paste("Average MSE for linear function: ", round(mse_linear, 3)))
# Compare bias and variance for cubic and linear models
bias_squared_cubic <- mean(f_hat_sum$bias_squared)
variance_cubic <- mean(f_hat_sum$variance)
print(paste("Average bias^2 for cubic model: ", round(bias_squared_cubic, 3)))
print(paste("Average variance for cubic model: ", round(variance_cubic, 3)))
bias_squared_linear <- mean(f_hat_sum_linear$bias_squared)
variance_linear <- mean(f_hat_sum_linear$variance)
print(paste("Average bias^2 for linear model: ", round(bias_squared_linear, 3)))
print(paste("Average variance for linear model: ", round(variance_linear, 3)))
library(tidyverse)
# Define f, the conditional expectation function (ground truth)
f <- function(x) -16 + 24*x - 9*x^2 + x^3
ggplot() +
geom_function(fun = f, xlim = c(0, 6), color = "orange", linewidth = 1.5)
# Define draw, a function that draws a data sample
#   at    A vector of predictors x. If NULL (the default), predictors are drawn randomly between 0 and 6
#   sd    The standard deviation of epsilon
draw <- function(at = NULL, n, sd, fun = f) {
if (is.null(at)) at <- runif(n = n, min = 0, max = 6)
tibble(x = at,
f = f(x),
epsilon = rnorm(n = length(x), mean = 0, sd = sd),
y = f(x) + epsilon)
}
# `draw` mode 1: draw a sample of 5 observations, using random values of x
set.seed(123)  # Setting seed for reproducibility
draw1 <- draw(n = 5, sd = 0)
print(draw1)
# `draw` mode 2: draw a sample where the predictors are fixed at values from 0 to 6 in increments of 0.5
draw2 <- draw(at = seq(0, 6, by = 0.5), n = NULL, sd = 10)
print(draw2)
# Plot f and y with a data sample of size 13, no noise (sd = 0)
no_noise_data <- draw(at = seq(0, 6, by = 0.5), n = NULL, sd = 0)
ggplot(no_noise_data, aes(x = x)) +
geom_line(aes(y = f), color = "orange", size = 1.5) +
geom_point(aes(y = y), color = "black", size = 3) +
ggtitle("No noise: f(x) vs y")
# Plot f and y with a data sample of size 13, more noise (sd = 10)
noise_data <- draw(at = seq(0, 6, by = 0.5), n = NULL, sd = 10)
ggplot(noise_data, aes(x = x)) +
geom_line(aes(y = f), color = "orange", size = 1.5) +
geom_point(aes(y = y), color = "black", size = 3) +
ggtitle("With noise: f(x) vs y")
# Define f_hat, the machine learning algorithm.
# Output
#   A data frame consisting of x from test_data and the corresponding predictions y_hat = f_hat(x)
f_hat <- function(test_data, training_data, deg) {
y_hat <- lm(y ~ poly(x, degree = deg, raw = TRUE), data = training_data) %>%
predict.lm(newdata = test_data)
mutate(test_data, y_hat = y_hat)
}
# Plot f with a training data and test data predictions
# Training and test data for no noise case
test_data <- draw(at = seq(0, 6, by = 0.5), n = NULL, sd = 0)
training_data <- draw(n = 10, sd = 0)
# Estimating cubic model (degree = 3)
f_hat_no_noise <- f_hat(test_data = test_data, training_data = training_data, deg = 3)
# Plotting
ggplot(test_data, aes(x = x)) +
geom_line(aes(y = f), color = "orange", size = 1.5) +
geom_line(data = f_hat_no_noise, aes(y = y_hat), color = "blue", size = 1.5, alpha = 0.5) +
geom_point(aes(y = y), color = "black", size = 3) +
ggtitle("No Noise: f(x) vs ML predictions (degree 3)")
# Plot f with a training data and test data predictions
# Training and test data for noise case
test_data_noise <- draw(at = seq(0, 6, by = 0.5), n = NULL, sd = 10)
training_data_noise <- draw(n = 10, sd = 10)
# Estimating cubic model (degree = 3)
f_hat_noise <- f_hat(test_data = test_data_noise, training_data = training_data_noise, deg = 3)
# Plotting
ggplot(test_data_noise, aes(x = x)) +
geom_line(aes(y = f), color = "orange", size = 1.5) +
geom_line(data = f_hat_noise, aes(y = y_hat), color = "blue", size = 1.5, alpha = 0.5) +
geom_point(aes(y = y), color = "black", size = 3) +
ggtitle("Noise (sd = 10): f(x) vs ML predictions (degree 3)")
# How many iterations (start with 2 iterations, then increase to 1,000 once the code is working)
iters <- 1000  # Number of iterations
f_hat_list <- vector("list", iters)
for (i in 1:iters) {
training_data <- draw(n = 10, sd = 10)
f_hat_list[[i]] <- f_hat(test_data = test_data_noise, training_data = training_data, deg = 3)
}
# Combine the results into one long data frame
f_hat_combined <- bind_rows(f_hat_list)
f_hat_sum <- f_hat_combined %>%
group_by(x) %>%
summarise(
y_hat_mean = mean(y_hat),
f_mean = mean(f),
mse = mean((y - y_hat)^2),
noise = mean(epsilon^2),
bias_squared = mean((f - y_hat_mean)^2),
variance = var(y_hat)
)
print(f_hat_sum)
ggplot(f_hat_sum, aes(x = x)) +
geom_line(aes(y = f_mean), color = "orange", size = 1.5) +
geom_line(aes(y = y_hat_mean), color = "blue", size = 1.5, alpha = 0.5) +
ggtitle("Summary of Predictions: f(x) vs y_hat_mean")
# Average MSE when estimating f as a cubic function of x
mse_cubic <- mean(f_hat_sum$mse)
print(paste("Average MSE for cubic function: ", round(mse_cubic, 3)))
# Repeat the ML process for degree = 1
f_hat_list_linear <- vector("list", iters)
for (i in 1:iters) {
training_data <- draw(n = 10, sd = 10)
f_hat_list_linear[[i]] <- f_hat(test_data = test_data_noise, training_data = training_data, deg = 1)
}
# Combine the results into one long data frame
f_hat_combined_linear <- bind_rows(f_hat_list_linear)
# Summarise the results for the linear model (deg = 1)
f_hat_sum_linear <- f_hat_combined_linear %>%
group_by(x) %>%
summarise(
y_hat_mean = mean(y_hat),
f_mean = mean(f),
mse = mean((y - y_hat)^2),
noise = mean(epsilon^2),
bias_squared = mean((f - y_hat_mean)^2),
variance = var(y_hat)
)
# Calculate the average MSE for the linear model (deg = 1)
mse_linear <- mean(f_hat_sum_linear$mse)
print(paste("Average MSE for linear function: ", round(mse_linear, 3)))
# Compare bias and variance for cubic and linear models
bias_squared_cubic <- mean(f_hat_sum$bias_squared)
variance_cubic <- mean(f_hat_sum$variance)
print(paste("Average bias^2 for cubic model: ", round(bias_squared_cubic, 3)))
print(paste("Average variance for cubic model: ", round(variance_cubic, 3)))
bias_squared_linear <- mean(f_hat_sum_linear$bias_squared)
variance_linear <- mean(f_hat_sum_linear$variance)
print(paste("Average bias^2 for linear model: ", round(bias_squared_linear, 3)))
print(paste("Average variance for linear model: ", round(variance_linear, 3)))
library(tidyverse)
library(tidyverse)
# Define f, the conditional expectation function (ground truth)
f <- function(x) -16 + 24*x - 9*x^2 + x^3
ggplot() +
geom_function(fun = f, xlim = c(0, 6), color = "orange", linewidth = 1.5)
# Define draw, a function that draws a data sample
#   at    A vector of predictors x. If NULL (the default), predictors are drawn randomly between 0 and 6
#   sd    The standard deviation of epsilon
draw <- function(at = NULL, n, sd, fun = f) {
if (is.null(at)) at <- runif(n = n, min = 0, max = 6)
tibble(x = at,
f = f(x),
epsilon = rnorm(n = length(x), mean = 0, sd = sd),
y = f(x) + epsilon)
}
# `draw` mode 1: draw a sample of 5 observations, using random values of x
set.seed(123)
draw1 <- draw(n = 5, sd = 0)
print(draw1)
# `draw` mode 2: draw a sample where the predictors are fixed at values from 0 to 6 in increments of 0.5
draw2 <- draw(at = seq(0, 6, by = 0.5), n = NULL, sd = 10)
print(draw2)
# Plot f and y with a data sample of size 13, no noise (sd = 0)
no_noise_data <- draw(at = seq(0, 6, by = 0.5), n = NULL, sd = 0)
ggplot(no_noise_data, aes(x = x)) +
geom_line(aes(y = f), color = "orange", size = 1.5) +
geom_point(aes(y = y), color = "black", size = 3) +
ggtitle("No noise: f(x) vs y")
# Plot f and y with a data sample of size 13, more noise (sd = 10)
noise_data <- draw(at = seq(0, 6, by = 0.5), n = NULL, sd = 10)
ggplot(noise_data, aes(x = x)) +
geom_line(aes(y = f), color = "orange", size = 1.5) +
geom_point(aes(y = y), color = "black", size = 3) +
ggtitle("With noise: f(x) vs y")
# Define f_hat, the machine learning algorithm.
# Output
#   A data frame consisting of x from test_data and the corresponding predictions y_hat = f_hat(x)
f_hat <- function(test_data, training_data, deg) {
y_hat <- lm(y ~ poly(x, degree = deg, raw = TRUE), data = training_data) %>%
predict.lm(newdata = test_data)
mutate(test_data, y_hat = y_hat)
}
# Plot f with a training data and test data predictions
test_data <- draw(at = seq(0, 6, by = 0.5), n = NULL, sd = 0)
training_data <- draw(n = 10, sd = 0)
# Estimating cubic model (degree = 3)
f_hat_no_noise <- f_hat(test_data = test_data, training_data = training_data, deg = 3)
# Plotting
ggplot(test_data, aes(x = x)) +
geom_line(aes(y = f), color = "orange", size = 1.5) +
geom_line(data = f_hat_no_noise, aes(y = y_hat), color = "blue", size = 1.5, alpha = 0.5) +
geom_point(aes(y = y), color = "black", size = 3) +
ggtitle("No Noise: f(x) vs ML predictions (degree 3)")
# Plot f with a training data and test data predictions
test_data_noise <- draw(at = seq(0, 6, by = 0.5), n = NULL, sd = 10)
training_data_noise <- draw(n = 10, sd = 10)
# Estimating cubic model (degree = 3)
f_hat_noise <- f_hat(test_data = test_data_noise, training_data = training_data_noise, deg = 3)
# Plotting
ggplot(test_data_noise, aes(x = x)) +
geom_line(aes(y = f), color = "orange", size = 1.5) +
geom_line(data = f_hat_noise, aes(y = y_hat), color = "blue", size = 1.5, alpha = 0.5) +
geom_point(aes(y = y), color = "black", size = 3) +
ggtitle("Noise (sd = 10): f(x) vs ML predictions (degree 3)")
# How many iterations (start with 2 iterations, then increase to 1,000 once the code is working)
iters <- 1000  # Number of iterations
f_hat_list <- vector("list", iters)
for (i in 1:iters) {
training_data <- draw(n = 10, sd = 10)
f_hat_list[[i]] <- f_hat(test_data = test_data_noise, training_data = training_data, deg = 3)
}
# Combine the results into one long data frame
f_hat_combined <- bind_rows(f_hat_list)
f_hat_sum <- f_hat_combined %>%
group_by(x) %>%
summarise(
y_hat_mean = mean(y_hat),
f_mean = mean(f),
mse = mean((y - y_hat)^2),
noise = mean(epsilon^2),
bias_squared = mean((f - y_hat_mean)^2),
variance = var(y_hat)
)
print(f_hat_sum)
ggplot(f_hat_sum, aes(x = x)) +
geom_line(aes(y = f_mean), color = "orange", size = 1.5) +
geom_line(aes(y = y_hat_mean), color = "blue", size = 1.5, alpha = 0.5) +
ggtitle("Summary of Predictions: f(x) vs y_hat_mean")
# Average MSE when estimating f as a cubic function of x
mse_cubic <- mean(f_hat_sum$mse)
print(paste("Average MSE for cubic function: ", round(mse_cubic, 3)))
# Repeat the ML process for degree = 1
f_hat_list_linear <- vector("list", iters)
for (i in 1:iters) {
training_data <- draw(n = 10, sd = 10)
f_hat_list_linear[[i]] <- f_hat(test_data = test_data_noise, training_data = training_data, deg = 1)
}
# Combine the results into one long data frame
f_hat_combined_linear <- bind_rows(f_hat_list_linear)
# Summarise the results for the linear model (deg = 1)
f_hat_sum_linear <- f_hat_combined_linear %>%
group_by(x) %>%
summarise(
y_hat_mean = mean(y_hat),
f_mean = mean(f),
mse = mean((y - y_hat)^2),
noise = mean(epsilon^2),
bias_squared = mean((f - y_hat_mean)^2),
variance = var(y_hat)
)
# Calculate the average MSE for the linear model (deg = 1)
mse_linear <- mean(f_hat_sum_linear$mse)
print(paste("Average MSE for linear function: ", round(mse_linear, 3)))
# Compare bias and variance for cubic and linear models
bias_squared_cubic <- mean(f_hat_sum$bias_squared)
variance_cubic <- mean(f_hat_sum$variance)
print(paste("Average bias^2 for cubic model: ", round(bias_squared_cubic, 3)))
print(paste("Average variance for cubic model: ", round(variance_cubic, 3)))
bias_squared_linear <- mean(f_hat_sum_linear$bias_squared)
variance_linear <- mean(f_hat_sum_linear$variance)
print(paste("Average bias^2 for linear model: ", round(bias_squared_linear, 3)))
print(paste("Average variance for linear model: ", round(variance_linear, 3)))
library(tidyverse)
# Define f, the conditional expectation function (ground truth)
f <- function(x) -16 + 24*x - 9*x^2 + x^3
ggplot() +
geom_function(fun = f, xlim = c(0, 6), color = "orange", linewidth = 1.5)
# Define draw, a function that draws a data sample
#   at    A vector of predictors x. If NULL (the default), predictors are drawn randomly between 0 and 6
#   sd    The standard deviation of epsilon
draw <- function(at = NULL, n, sd, fun = f) {
if (is.null(at)) at <- runif(n = n, min = 0, max = 6)
tibble(x = at,
f = f(x),
epsilon = rnorm(n = length(x), mean = 0, sd = sd),
y = f(x) + epsilon)
}
# `draw` mode 1: draw a sample of 5 observations, using random values of x
set.seed(123)
draw1 <- draw(n = 5, sd = 0)
print(draw1)
# `draw` mode 2: draw a sample where the predictors are fixed at values from 0 to 6 in increments of 0.5
draw2 <- draw(at = seq(0, 6, by = 0.5), n = NULL, sd = 10)
print(draw2)
# Plot f and y with a data sample of size 13, no noise (sd = 0)
no_noise_data <- draw(at = seq(0, 6, by = 0.5), n = NULL, sd = 0)
ggplot(no_noise_data, aes(x = x)) +
geom_line(aes(y = f), color = "orange", size = 1.5) +
geom_point(aes(y = y), color = "black", size = 3) +
ggtitle("No noise: f(x) vs y")
# Plot f and y with a data sample of size 13, more noise (sd = 10)
noise_data <- draw(at = seq(0, 6, by = 0.5), n = NULL, sd = 10)
ggplot(noise_data, aes(x = x)) +
geom_line(aes(y = f), color = "orange", size = 1.5) +
geom_point(aes(y = y), color = "black", size = 3) +
ggtitle("With noise: f(x) vs y")
# Define f_hat, the machine learning algorithm.
# Output
#   A data frame consisting of x from test_data and the corresponding predictions y_hat = f_hat(x)
f_hat <- function(test_data, training_data, deg) {
y_hat <- lm(y ~ poly(x, degree = deg, raw = TRUE), data = training_data) %>%
predict.lm(newdata = test_data)
mutate(test_data, y_hat = y_hat)
}
# Plot f with a training data and test data predictions
test_data <- draw(at = seq(0, 6, by = 0.5), n = NULL, sd = 0)
training_data <- draw(n = 10, sd = 0)
# Estimating cubic model (degree = 3)
f_hat_no_noise <- f_hat(test_data = test_data, training_data = training_data, deg = 3)
# Plotting
ggplot(test_data, aes(x = x)) +
geom_line(aes(y = f), color = "orange", size = 1.5) +
geom_line(data = f_hat_no_noise, aes(y = y_hat), color = "blue", size = 1.5, alpha = 0.5) +
geom_point(aes(y = y), color = "black", size = 3) +
ggtitle("No Noise: f(x) vs ML predictions (degree 3)")
# Plot f with a training data and test data predictions
test_data_noise <- draw(at = seq(0, 6, by = 0.5), n = NULL, sd = 10)
training_data_noise <- draw(n = 10, sd = 10)
# Estimating cubic model (degree = 3)
f_hat_noise <- f_hat(test_data = test_data_noise, training_data = training_data_noise, deg = 3)
# Plotting
ggplot(test_data_noise, aes(x = x)) +
geom_line(aes(y = f), color = "orange", size = 1.5) +
geom_line(data = f_hat_noise, aes(y = y_hat), color = "blue", size = 1.5, alpha = 0.5) +
geom_point(aes(y = y), color = "black", size = 3) +
ggtitle("Noise (sd = 10): f(x) vs ML predictions (degree 3)")
# How many iterations (start with 2 iterations, then increase to 1,000 once the code is working)
iters <- 1000  # Number of iterations
f_hat_list <- vector("list", iters)
for (i in 1:iters) {
training_data <- draw(n = 10, sd = 10)
f_hat_list[[i]] <- f_hat(test_data = test_data_noise, training_data = training_data, deg = 3)
}
# Combine the results into one long data frame
f_hat_combined <- bind_rows(f_hat_list)
f_hat_sum <- f_hat_combined %>%
group_by(x) %>%
summarise(
y_hat_mean = mean(y_hat),
f_mean = mean(f),
mse = mean((y - y_hat)^2),
noise = mean(epsilon^2),
bias_squared = mean((f - y_hat_mean)^2),
variance = var(y_hat)
)
print(f_hat_sum)
ggplot(f_hat_sum, aes(x = x)) +
geom_line(aes(y = f_mean), color = "orange", size = 1.5) +
geom_line(aes(y = y_hat_mean), color = "blue", size = 1.5, alpha = 0.5) +
ggtitle("Summary of Predictions: f(x) vs y_hat_mean")
# Average MSE when estimating f as a cubic function of x
mse_cubic <- mean(f_hat_sum$mse)
print(paste("Average MSE for cubic function: ", round(mse_cubic, 3)))
# Repeat the ML process for degree = 1
f_hat_list_linear <- vector("list", iters)
for (i in 1:iters) {
training_data <- draw(n = 10, sd = 10)
f_hat_list_linear[[i]] <- f_hat(test_data = test_data_noise, training_data = training_data, deg = 1)
}
# Combine the results into one long data frame
f_hat_combined_linear <- bind_rows(f_hat_list_linear)
# Summarise the results for the linear model (deg = 1)
f_hat_sum_linear <- f_hat_combined_linear %>%
group_by(x) %>%
summarise(
y_hat_mean = mean(y_hat),
f_mean = mean(f),
mse = mean((y - y_hat)^2),
noise = mean(epsilon^2),
bias_squared = mean((f - y_hat_mean)^2),
variance = var(y_hat)
)
# Calculate the average MSE for the linear model (deg = 1)
mse_linear <- mean(f_hat_sum_linear$mse)
print(paste("Average MSE for linear function: ", round(mse_linear, 3)))
# Compare bias and variance for cubic and linear models
bias_squared_cubic <- mean(f_hat_sum$bias_squared)
variance_cubic <- mean(f_hat_sum$variance)
print(paste("Average bias^2 for cubic model: ", round(bias_squared_cubic, 3)))
print(paste("Average variance for cubic model: ", round(variance_cubic, 3)))
bias_squared_linear <- mean(f_hat_sum_linear$bias_squared)
variance_linear <- mean(f_hat_sum_linear$variance)
print(paste("Average bias^2 for linear model: ", round(bias_squared_linear, 3)))
print(paste("Average variance for linear model: ", round(variance_linear, 3)))
