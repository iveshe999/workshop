---
title: "Lab-06: Machine Learning, Bias, and Variance"
author: Ives He
output:
  html_document:
    theme: simplex
    fig_caption: true
---

# Getting started
In this exercise you will create a simulation in which you understand how the (simulated) world works. Using data that are generated by this world, you will use machine learning (ML) to try to reverse engineer how the world works. You will calculate the mean squared error (MSE) of different ML algorithms and decompose the MSE into noise, bias, and variance components.

Start by loading the `tidyverse` package.

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
```

# Simulation Setup

We will build a world in which outcomes $Y$ are related to a predictor variable $X$ according to the equation $Y = f(X) + \epsilon,$ where $\epsilon$ is a mean-zero normally distributed random variable and

$$ f(X) = -16 + 24X - 9X^2 + X^3. $$
Remember that $f$ is the conditional expectation function, describing the expected value of $Y$ given $X$. The following code chunk defines and plots this function for values of $X$ ranging from 0 to 6.

```{r}
# Define f, the conditional expectation function (ground truth)
f <- function(x) -16 + 24*x - 9*x^2 + x^3
ggplot() +
  geom_function(fun = f, xlim = c(0, 6), color = "orange", linewidth = 1.5)
```

We need our world to generate data. In some cases, we'll want to collect data where the world picks values of $X$ along with corresponding values of $Y$. In other cases, we might want to collect data for specific values of $X$, and let the world generate only the corresponding $Y$ values.

The function in the following code chunk can perform either task. The function always returns a data frame that contains the following variables:

- `x` -- The value of the predictor variable $X$ for each observation in the dataset
- `f` -- The true expected value of $Y$ given $X$ (we can only calculate this in a simulation, where we are omnicient)
- `epsilon` -- The noise term $\epsilon$ that causes realized outcomes to deviate from their expected values
- `y` -- The realized outcome, defined by $y = f(x) + \epsilon$

The function has two modes:

1. If you set the `at` argument equal to `NULL` and provide a positive value for `n`, the function will create a sample of `n` observations, and for each observation will randomly select a characteristic `x` in the interval 0 to 6.
2. If you set the `at` argument to be a vector of `x` characteristics, the function will create a sample of `n = length(x)` observations with characteristics defined by `x = at`.

Finally, you get to determine how much noise there is in this world via the `sd` argument, which sets the standard deviation used when randomly drawing values for the error term `epsilon`. For example, if you set `sd = 0`, there will be no noise, and outcomes will be perfectly determined given `x`. By contrast, when `sd` is large, outcomes will be only partly detmined by `x`.

```{r}
# Define draw, a function that draws a data sample
#   at    A vector of predictors x. If NULL (the default), predictors are drawn randomly between 0 and 6
#   sd    The standard deviation of epsilon
draw <- function(at = NULL, n, sd, fun = f) {
  if (is.null(at)) at <- runif(n = n, min = 0, max = 6) 
  tibble(x = at,
         f = f(x),
         epsilon = rnorm(n = length(x), mean = 0, sd = sd),
         y = f(x) + epsilon)
}
```


# Problem 1: Draw data samples

This problem helps you gain familiarity with how to draw data samples in the simulation using the `draw` function.

First, collect a data sample of `n = 5` observations, where the characteristics `x` are selected randomly from the interval 0 to 6 (i.e., using mode 1 described above). Set the standard deviation of `epsilon` equal to zero. Print the contents of this data frame.

```{r}
# `draw` mode 1: draw a sample of 5 observations, using random values of x
set.seed(123)
draw1 <- draw(n = 5, sd = 0)
print(draw1)
```

Next, collect a data sample where the characteristics `x` are the sequence of values from 0 to 6 in increments of 0.5. Set the standard deviation of `epsilon` equal to 10. Print the contents of this data frame.

```{r}
# `draw` mode 2: draw a sample where the predictors are fixed at values from 0 to 6 in increments of 0.5
draw2 <- draw(at = seq(0, 6, by = 0.5), n = NULL, sd = 10)
print(draw2)
```

# Problem 2: Visualizing noise

In this problem, use the `draw` function along with `ggplot2` to visualize how noise obscures $f$, the systematic relationship between $X$ and $Y$. 

### No noise
In the following code chunk, collect a data sample where the characteristics `x` are the sequence of values from 0 to 6 in increments of 0.5. Set the standard deviation of `epsilon` equal to zero. Create a single ggplot that has the following two layers:

1. First layer. A line plot of how `f` (the true expected value of `y` given `x`) varies with `x`. Set the line color to be orange, and size to be 1.5.
2. Second layer. A scatter plot of how `y` (the realized outcome) varies with `x`. Set the point color to be black, and size to be 3.

```{r}
# Plot f and y with a data sample of size 13, no noise (sd = 0)
no_noise_data <- draw(at = seq(0, 6, by = 0.5), n = NULL, sd = 0)

ggplot(no_noise_data, aes(x = x)) +
  geom_line(aes(y = f), color = "orange", size = 1.5) +
  geom_point(aes(y = y), color = "black", size = 3) +
  ggtitle("No noise: f(x) vs y")

```


### More noise
Repeat the exercise from the "no noise" scenario, except now set the standard deviation of `epsilon` to be equal to 10. 

```{r}
# Plot f and y with a data sample of size 13, more noise (sd = 10)
noise_data <- draw(at = seq(0, 6, by = 0.5), n = NULL, sd = 10)

ggplot(noise_data, aes(x = x)) +
  geom_line(aes(y = f), color = "orange", size = 1.5) +
  geom_point(aes(y = y), color = "black", size = 3) +
  ggtitle("With noise: f(x) vs y")

```


### Discuss
In which world do you expect machine learning to perform better? How would your conclusions change, if at all, if you collected data samples where the characteristics `x` were selected randomly instead of at fixed intervals? Discuss briefly, connecting your discussion to the patterns revealed in your plots.

Machine learning will perform better in the no-noise scenario because the relationship between x and y is clear and easier to model. In the noisy world, performance declines due to difficulty in distinguishing noise from the signal. Random sampling of x may improve generalization but adds complexity, especially in noisy environments.

# Problem 3: The Machine

### Setup 

In this problem, we will use machine learning to construct an estimate of $f$, based only on values of `x` and `y` observed in data samples generated by this world. The machine learning algorithm will employ linear regression. Despite its name, linear regression can be used to estimate many types of nonlinear relationships, including polynomial relationships. 

We will use the following R functions to estimate $f$ using linear regression and to make predictions. You can look up the help documentation for each to learn more.

- `lm` -- Fits linear models
- `poly` -- Constructs polynomials of a specified degree
   - Example 1: `y ~ poly(x, degree = 1, raw = TRUE)` builds a simple linear model of the form $y = \beta_0 + \beta_1 x + \epsilon$
   - Example 2: `y ~ poly(x, degree = 3, raw = TRUE)` builds a cubic model of the form $y = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \epsilon$
- `predict.lm` -- Uses the output of `lm` to predict the outcome for specified values of `x`

Our machine learning algorithm has three different arguments:

1. `test_data` -- The dataset, produced by `draw`, for which we want to predict the outcomes.
2. `training_data` -- The dataset, produced by `draw`, that will we use to estimate $f$ via linear regression.
3. `deg` -- Specifies the degree of the polynomial in $x$ for the estimate of `f`.

The function `f_hat` implements this algorithm. Given the needed inputs, the function returns a data frame equal to `test_data`, but with an additional column `y_hat` of the predicted outcomes.

```{r}
# Define f_hat, the machine learning algorithm. 
# Output
#   A data frame consisting of x from test_data and the corresponding predictions y_hat = f_hat(x)
f_hat <- function(test_data, training_data, deg) {
  y_hat <- lm(y ~ poly(x, degree = deg, raw = TRUE), data = training_data) %>% 
    predict.lm(newdata = test_data)
  mutate(test_data, y_hat = y_hat)
}
```

### ML under no noise 

Run the ML algorithm under a scenario where the world is deterministic (no noise). To do so, implement the algorithm with the following inputs:

- `test_data` -- Collect a data sample where the characteristics `x` are the sequence of values from 0 to 6 in increments of 0.5 and the standard deviation of `epsilon` equal to zero.
- `training_data` -- Collect a data sample of `n = 10` observations, where the characteristics `x` are selected randomly and the standard deviation of `epsilon` equal to zero.
- `deg` -- Set `f_hat` to be a cubic function of `x` (`deg = 3`).

Using the result generated by `f_hat`, create a single ggplot that has the following three layers:

1. First layer. A line plot of how `f` varies with `x`. Set the line color to be orange, and size to be 1.5.
2. Second layer. A line plot of how `y_hat` varies with `x`. Set the point color to be blue, size to be 3, and transparency to be 1/2.
3. Third layer. A scatter plot of how `y` varies with `x`. Set the point color to be black, and size to be 3.

```{r}
# Plot f with a training data and test data predictions
test_data <- draw(at = seq(0, 6, by = 0.5), n = NULL, sd = 0)
training_data <- draw(n = 10, sd = 0)

# Estimating cubic model (degree = 3)
f_hat_no_noise <- f_hat(test_data = test_data, training_data = training_data, deg = 3)

# Plotting
ggplot(test_data, aes(x = x)) +
  geom_line(aes(y = f), color = "orange", size = 1.5) +
  geom_line(data = f_hat_no_noise, aes(y = y_hat), color = "blue", size = 1.5, alpha = 0.5) +
  geom_point(aes(y = y), color = "black", size = 3) +
  ggtitle("No Noise: f(x) vs ML predictions (degree 3)")


```

**Discuss:** How well did the prediction algorithm perform? Is there any bias or variance in this case? Would that change if you had forced `f_hat` estimate a simple linear function of `x` (i.e., set `deg = 1`)?

The prediction algorithm performs well in this case, as the cubic model (degree 3) closely fits the true relationship between f(x) and y, with minimal bias or variance. If the degree of the polynomial were reduced to 1, forcing a simple linear model, there would be significant bias, as the linear model would underfit the true nonlinear relationship. This would result in poor prediction performance.

### ML under noise 

Repeat the exercise of "ML under no noise," except this time add noise to the world by setting the standard deviation of $\epsilon$ to be equal to 10 when drawing the `test_data` and `training_data`. How do the results and your discussion points change?

```{r}
# Plot f with a training data and test data predictions
test_data_noise <- draw(at = seq(0, 6, by = 0.5), n = NULL, sd = 10)
training_data_noise <- draw(n = 10, sd = 10)

# Estimating cubic model (degree = 3)
f_hat_noise <- f_hat(test_data = test_data_noise, training_data = training_data_noise, deg = 3)

# Plotting
ggplot(test_data_noise, aes(x = x)) +
  geom_line(aes(y = f), color = "orange", size = 1.5) +
  geom_line(data = f_hat_noise, aes(y = y_hat), color = "blue", size = 1.5, alpha = 0.5) +
  geom_point(aes(y = y), color = "black", size = 3) +
  ggtitle("Noise (sd = 10): f(x) vs ML predictions (degree 3)")


```

In the noisy case, the prediction algorithm's performance worsens as the cubic model attempts to fit the noise in the data, leading to high variance. The model may overfit the training data, capturing random fluctuations rather than the true underlying pattern. If we reduced the degree to 1, the model would still struggle with bias, underfitting the true curve and performing poorly overall, though potentially reducing overfitting. The key challenge is balancing bias and variance in the presence of noise.

# Problem 4: MSE

As a final step, you will calculate MSE from the machine learning algorithm. To do so, repeat the prediction part (not the visualization part) of the previous exercise "ML under noise" many times (i.e., `deg = 3`, `sd = 10`, etc.). Collect the prediction results as you go. Finally, using this collection of results, calculate how well the ML algorithm performed on average.

### Repeat the ML algorithm many times

In the following code chunk, use a `for` loop to repeat the prediction exercise 1,000 times. Save the output from the $i^\textit{th}$ iteration of the loop in the $i^\textit{th}$ position of the list `f_hat_list`. In the end, each element of this list will be the data frame returned by `f_hat`.

```{r}
# How many iterations (start with 2 iterations, then increase to 1,000 once the code is working)
iters <- 1000  # Number of iterations
f_hat_list <- vector("list", iters)

for (i in 1:iters) {
  training_data <- draw(n = 10, sd = 10)
  f_hat_list[[i]] <- f_hat(test_data = test_data_noise, training_data = training_data, deg = 3)
}

# Combine the results into one long data frame
f_hat_combined <- bind_rows(f_hat_list)
```

### Assess the quality of predictions

Now that you have the results of estimating the ML algorithm many times, create a data frame called `f_hat_sum` that summarizes prediction results, separately for each value of `x` in `test_data`. (Hints: combine the list of data frames into one long data frame using `bind_rows`, then use `group_by` and `summarise` as appropriate to create the requested summaries.)

The summary data frame should contain one row for each value of `x` in `test_data` and should contain the following variables:

- `y_hat_mean` -- The mean predicted outcome
- `f_mean` -- The mean of the `f` (the true expected value, for use as a benchmark)
- `mse` -- The mean squared error of predictions
- `noise` -- The mean of `epsilon` squared
- `bias_squared` -- The bias in the predictions, squared
- `variance` -- The variance in the predictions

Add code to the following code chunk to build this summary data set, and then print the contents. 

```{r message=FALSE, warning=FALSE}
f_hat_sum <- f_hat_combined %>%
  group_by(x) %>%
  summarise(
    y_hat_mean = mean(y_hat),
    f_mean = mean(f),
    mse = mean((y - y_hat)^2),
    noise = mean(epsilon^2),
    bias_squared = mean((f - y_hat_mean)^2),
    variance = var(y_hat)
  )

print(f_hat_sum)

```
Using the results, briefly discuss the following:

- Is it true that MSE = Noise + Bias^2 + Variance?

Yes, the MSE can be decomposed into Noise, Bias², and Variance. The formula holds, although variance is not explicitly listed in your table.

- Is MSE the same at all values of `x`?

No, MSE varies across x values. For example, at x = 0.0, MSE is 2259.09, while at x = 2.0, it drops to 105.35.

- What component (noise, bias, or variance) is driving most of the MSE? Does it depend on the value of `x`?

The contribution of Noise and Bias² varies with x. Noise dominates at some points, like x = 1.5, while Bias² plays a role in others, like x = 0.0. It depends on the value of x.

### Visualize

Visualize the summary of prediction results. Using the summary data frame you just constructed, create a ggplot with the following two layers:

1. First layer. A line plot of how `f_mean` (which is the same as `f`) varies with `x`. Set the line color to be orange, and size to be 1.5.
2. Second layer. A line plot of how `y_hat_mean` varies with `x`. Set the point color to be blue, size to be 3, and transparency to be 1/2.

```{r}
ggplot(f_hat_sum, aes(x = x)) +
  geom_line(aes(y = f_mean), color = "orange", size = 1.5) +
  geom_line(aes(y = y_hat_mean), color = "blue", size = 1.5, alpha = 0.5) +
  ggtitle("Summary of Predictions: f(x) vs y_hat_mean")
```

**Discuss:** What does this plot indicate about bias in the ML algorithm? What about variance?

The plot shows low bias because the predicted values (blue line) closely match the true function (orange line), meaning the model makes accurate predictions. There is moderate variance, especially at the boundaries (around
x=0 and x=6), where the predictions diverge slightly from the true function. This is likely due to fewer data points near the edges, making the model more sensitive to variations in those regions.

# Problem 5 [optional]: Bias-Variance Tradeoff

On average, over values of $X$ from 0 to 6, and under the "ML under noise" (`sd` = 10) scenario, do you get a larger or smaller MSE when the ML algorithm estimates $f$ as a cubic function of $X$ (`deg = 3`, as done in Problem 4) or as a simple linear function of $X$ (`deg = 1`)? Which version of the algorithm leads to higher bias, and which to higher variance?

Discuss these findings and evaluate the following claim: if the true `f` is nonlinear, then a simple linear models will generally perform worse than a nonlinear model.

```{r message=FALSE, warning=FALSE}
# Average MSE when estimating f as a cubic function of x
mse_cubic <- mean(f_hat_sum$mse)
print(paste("Average MSE for cubic function: ", round(mse_cubic, 3)))
# Repeat the ML process for degree = 1
f_hat_list_linear <- vector("list", iters)

for (i in 1:iters) {
  training_data <- draw(n = 10, sd = 10)
  f_hat_list_linear[[i]] <- f_hat(test_data = test_data_noise, training_data = training_data, deg = 1)
}

# Combine the results into one long data frame
f_hat_combined_linear <- bind_rows(f_hat_list_linear)

# Summarise the results for the linear model (deg = 1)
f_hat_sum_linear <- f_hat_combined_linear %>%
  group_by(x) %>%
  summarise(
    y_hat_mean = mean(y_hat),
    f_mean = mean(f),
    mse = mean((y - y_hat)^2),
    noise = mean(epsilon^2),
    bias_squared = mean((f - y_hat_mean)^2),
    variance = var(y_hat)
  )

# Calculate the average MSE for the linear model (deg = 1)
mse_linear <- mean(f_hat_sum_linear$mse)
print(paste("Average MSE for linear function: ", round(mse_linear, 3)))

# Compare bias and variance for cubic and linear models
bias_squared_cubic <- mean(f_hat_sum$bias_squared)
variance_cubic <- mean(f_hat_sum$variance)
print(paste("Average bias^2 for cubic model: ", round(bias_squared_cubic, 3)))
print(paste("Average variance for cubic model: ", round(variance_cubic, 3)))

bias_squared_linear <- mean(f_hat_sum_linear$bias_squared)
variance_linear <- mean(f_hat_sum_linear$variance)
print(paste("Average bias^2 for linear model: ", round(bias_squared_linear, 3)))
print(paste("Average variance for linear model: ", round(variance_linear, 3)))


```

MSE: If the true function f is cubic, we expect the cubic model (deg=3) to have a lower MSE compared to the linear model (deg=1).

Bias: The linear model typically has higher bias because it oversimplifies the relationship between x and y. A cubic model can capture the nonlinearity in the data, leading to lower bias.

Variance: The cubic model will likely have higher variance compared to the linear model because it is more flexible and sensitive to the training data, especially in the presence of noise.