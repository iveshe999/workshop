---
title: "Lab 08 -- Multiple Linear Regression"
author: Ives He
output:
  html_document:
  theme: simplex
  fig_caption: true
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# Getting started

In this assignment, you will apply multiple regression tools to the Boston Housing Data, which consist of housing values in suburbs of Boston taken from the 1970 Census. The data set also contains other information that may affect house prices, such as the crime rate in the area and the proportion of owner-occupied houses. The data set and data dictionary can be found at [Boston Housing Data](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/).

In RStudio, create a folder called `lab-08` and set this folder as your working directory. Run the following commands to load a dataset of housing values into `data_housing`. Here is a table that lists some of the variable definitions:

| Variable name | Definition |
|:-|:-|
| CRIM          |  Per capita crime rate by census tract |
| ZN            |  Proportion of residential land zoned for lots over 25,000 sq.ft.  |
| INDUS         | Proportion of non-retail business acres per town   |
| NOX           |  Nitric oxides concentration (parts per 10 million)  |
| RM            |  Average number of rooms per dwelling  |
| AGE           |  Proportion of owner-occupied units built prior to 1940  |
| DIS           |  Weighted distances to five Boston employment centers  |
| RAD           | Index of accessibility to radial highways   |
| TAX           | Full-value property-tax rate per $10,000   |
| PTRATIO       | Pupil-teacher ratio by town   |
| LSTAT         | % lower status of the population   |
| MEDV          | Median value of owner-occupied homes in $1000's   |

```{r, message=FALSE, warning=FALSE}
library(tidyverse)

# Variable names
variables <- c("CRIM", "ZN", "INDUS",  "CHAS", "NOX", "RM", "AGE", 
               "DIS", "RAD", "TAX", "PTRATIO", "B", "LSTAT", "MEDV")

# Read in the data
housing_data <- read_table("/Users/iveshe/Library/Mobile Documents/com~apple~CloudDocs/Term 1/FIN 550/Lab/lab-08/housing-lab08.data", col_names = variables)
```


# Problem 1: Linear regression

1. Using `ggplot`, create a scatter plot with the median value of owner-occupied homes (in $1000's) on the vertical axis and the weighted distances to five Boston employment centers on the horizontal axis. Add a linear trendline to the graph using the `geom_smooth` command. Does the graph suggest that the systematic relationship between these two variables is linear or non-linear? Briefly discuss.
```{r}
# Scatter plot
ggplot(housing_data, aes(x = DIS, y = MEDV)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(title = "Median Home Value vs Distance to Employment Centers",
       x = "Distance to Employment Centers (DIS)",
       y = "Median Home Value (MEDV) in $1000's")
```

2. Estimate a simple linear model where the outcome is the median value of owner-occupied homes (in $1000's) and the explanatory variable is the weighted distances to five Boston employment centers. Save the result of this regression in a variable called `lm_dis` and display the results using the `stargazer` function.
```{r}
library(stargazer)
# Simple linear regression
lm_dis <- lm(MEDV ~ DIS, data = housing_data)
# Show regression output using stargazer function
stargazer(lm_dis, type = "text")
```


3. We often describe an estimate as "statistically significant" at the 95% confidence level if the estimate is more than 1.96 (i.e., about 2) standard errors away from zero. Is the estimated parameter on the distance variable statistically significant? 

```{r}
summary(lm_dis)
```

Yes, the estimated parameter for the distance variable (DIS) is statistically significant at the 95% confidence level. The coefficient for DIS is 1.092 with a standard error of 0.188, yielding a t-value of approximately 5.8, which is well above 1.96, confirming statistical significance.

4. Is the following a valid interpretation of the regression results? "Median home values tend to be higher, by about \$1,000 per mile, for neighborhoods located further from employment centers." Briefly discuss.

The interpretation is partially valid but requires clarification. The regression coefficient for DIS is 1.092, meaning that for every unit increase in DIS (which represents a weighted distance measure, not necessarily miles), median home values increase by approximately $1,092. However, this interpretation assumes a linear relationship and doesn't account for potential other factors or non-linearities. Therefore, it's important to note that the relationship may be more complex than just "per mile."

5. Is the following a valid interpretation of the regression results? "All else equal, being located further from employment centers causes median home values to decrease." Briefly discuss.

This interpretation is not valid. The regression results show that the coefficient for DIS is positive (1.092), meaning that as the distance from employment centers increases, median home values increase, not decrease. Therefore, being located further from employment centers is associated with higher home values, not lower.

# Problem 2: Mean Squared Error (MSE)

1. What is the "mean squared error" (MSE) of the `lm_dis` model you have estimated? Hint: MSE is calculated as the average value of squared prediction errors for all observations in the data.
```{r}
# Calculate MSE of the model
mse_dis <- mean((housing_data$MEDV - predict(lm_dis))^2)
mse_dis
```

2. Add to the `housing_data` data frame 10 new variables called `random_var1` - `random_var10`, where each variable is defined to be a random number drawn uniformly from the interval from -1 to 1. Estimate a linear model named `lm_plus` that builds on the `lm_dis` model, but adds the `random_varX` variables as additional explanatory variable. How does the MSE of `lm_plus` compare to that of `lm_dis`? What do these results imply about using MSE as a criterion for selecting whether a model with more predictors is better than a simpler model with fewer controls?
```{r}
# Set the seed for replicability
set.seed(42)
# Add random_var to the data frame
for (i in 1:10) {
  housing_data[[paste0("random_var", i)]] <- runif(nrow(housing_data), -1, 1)
}

# Estimate lm_plus model
lm_plus <- lm(MEDV ~ DIS + random_var1 + random_var2 + random_var3 + random_var4 + 
                random_var5 + random_var6 + random_var7 + random_var8 + random_var9 + random_var10, 
              data = housing_data)

# Calculate lm_plus MSE
mse_plus <- mean((housing_data$MEDV - predict(lm_plus))^2)
mse_plus
```

The MSE for lm_plus (77.90) is slightly lower than for lm_dis (79.15), despite the additional predictors being random. This shows that adding predictors can reduce MSE but may lead to overfitting. MSE alone is insufficient; consider adjusted R-squared and predictor significance to ensure model quality.

# Problem 3: Multiple linear regression

1. Building on the simple linear model `lm_dis` from Problem 1, estimate a multiple linear regression of `MEDV` that also controls for the full-value property-tax rate per $10,000. Save the result of this regression in a variable called `lm_tax`.

```{r, warning=FALSE}
# Estimate the model
lm_tax <- lm(MEDV ~ DIS + TAX, data = housing_data)
```

2. Building on the linear model `lm_tax`, estimate a multiple linear regression of `MEDV` that also controls for nitric oxides concentration (parts per 10 million). Save the result of this regression in a variable called `lm_nox`. 

```{r, warning=FALSE}
# Estimate the model
lm_nox <- lm(MEDV ~ DIS + TAX + NOX, data = housing_data)
```

3. Building on the simple linear model `lm_nox`, estimate a multiple linear regression of `MEDV` that also controls for percent of the population with low socioeconomic status. Save the result of this regression in a variable called `lm_ses`. 

```{r, warning=FALSE}
# Estimate the model
lm_ses <- lm(MEDV ~ DIS + TAX + NOX + LSTAT, data = housing_data)
```

4. Report regressions results from all four models above in the same table using the `stargazer()` command. What can we learn from these regressions about the relationship between distance to employment centers and median home values?

```{r, warning=FALSE}
stargazer(lm_dis, lm_tax, lm_nox, lm_ses, type = "text")
```

In Model 1, DIS has a positive coefficient of 1.092. However, in Models 2-4, after controlling for TAX, NOX, and LSTAT, the DIS coefficient becomes negative, ranging from -0.003 to -1.206. This indicates that when controlling for these factors, greater distance from employment centers lowers home values.

# Problem 4: Non-linear Regression

1. Estimate a quartic (4th degree polynomial) relationship between median home values and distance to employment centers. Do this "manually" by constructing each of the polynomial terms as new variables in the data frame. Assign the regression results to `lm_dis_man`. Create a scatter plot of median home values versus distance, and add a line plot layer showing the predicted values from this model. Choose a nice color for this line plot (e.g., firebrick red) to help it to stand out against the black scatter plot markers.
```{r}
# Add DIS_2-DIS_4 to the data frame
housing_data <- housing_data %>%
  mutate(DIS_2 = DIS^2, DIS_3 = DIS^3, DIS_4 = DIS^4)

# Estimate the model
lm_dis_man <- lm(MEDV ~ DIS + DIS_2 + DIS_3 + DIS_4, data = housing_data)

# Scatter plot with model predicted values
ggplot(housing_data, aes(x = DIS, y = MEDV)) +
  geom_point() +
  geom_line(aes(y = predict(lm_dis_man)), color = "firebrick") +
  labs(title = "Quartic Model: Median Home Value vs Distance to Employment Centers",
       x = "Distance to Employment Centers (DIS)",
       y = "Median Home Value (MEDV) in $1000's")
```

2. Use the `poly()` function in the regression formula to estimate the same quartic model in the previous question. For this question, you should not construct any new variables yourself. Save the results to a variable called `lm_dis_poly`. Use the `all.equal()` command to confirm that the estimated coefficients and predicted values from the `lm_dis_man` and `lm_dis_poly` models are the same.
```{r}
# Estimate the model
lm_dis_poly <- lm(MEDV ~ poly(DIS, 4), data = housing_data)

# Confirm that lm_dis_man and lm_dis_poly coefficients are equal
all.equal(coef(lm_dis_man), coef(lm_dis_poly))

# Confirm that lm_dis_man and lm_dis_poly predicted values are equal
all.equal(predict(lm_dis_man), predict(lm_dis_poly))
```


3. The `lm_dis_man` and `lm_ses` models both have 4 predictor variables. Calculate the MSE for each model. Do you think the model with a lower MSE is better than the model with the higher MSE? How does your answer compare to or differ from your answer to Problem 2.2 above? Briefly discuss.
```{r}
mse_dis_man <- mean((housing_data$MEDV - predict(lm_dis_man))^2)
mse_ses <- mean((housing_data$MEDV - predict(lm_ses))^2)

mse_dis_man
mse_ses
```

The model lm_ses has a much lower MSE (35.03) than lm_dis_man (75.55), indicating a better fit. However, as seen in Problem 2.2, a lower MSE alone isn't always better if the added variables are irrelevant or overfitting. In this case, lm_ses is superior because it includes meaningful predictors like LSTAT, whereas lm_dis_man relies on polynomial terms that don't add significant real-world interpretability.